{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터가 부족한 상황이라면 모델에 파이토치의 nn.Embedding()을 사용하는 것보다 다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택일 수 있습니다. 일반적이고 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB 리뷰 데이터를 훈련 데이터로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 지정된 프로시저를 찾을 수 없습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m data, datasets\n\u001b[0;32m      3\u001b[0m TEXT \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mField(sequential\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m LABEL \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mField(sequential\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[39m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(_get_torch_home(), \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _mod_utils\u001b[39m.\u001b[39mis_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchtext._torchtext\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtorchtext C++ Extension is not found.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m _load_lib(\u001b[39m\"\u001b[39;49m\u001b[39mlibtorchtext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torch\\_ops.py:104\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     99\u001b[0m path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils_internal\u001b[39m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m    100\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] 지정된 프로시저를 찾을 수 없습니다"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련 데이터의 크기 : {}' .format(len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(trainset[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토치텍스트를 사용한 사전 훈련된 워드 임베딩"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 훈련된 Word2Vec 모델 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v') # '영어/한국어 Word2Vec 훈련하기 챕터'에서 만들어두었던 'eng_w2v' 모델을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_model['this']) # 영어 단어 'this'의 임베딩 벡터값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_model['self-indulgent']) # 영어 단어 'self-indulgent'의 임베딩 벡터값 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 훈련된 Word2Vec을 초기 임베딩으로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "vectors = Vectors(name=\"eng_w2v\") # 사전 훈련된 Word2Vec 모델을 vectors에 저장\n",
    "\n",
    "TEXT.build_vocab(trainset, vectors=vectors, max_size=10000, min_freq=10) # 훈련 데이터의 단어 집합(vocabulary)를 만드는 것과 동시에 임베딩 벡터값들을 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi) # 현재 단어 집합의 단어와 맵핑된 고유한 정수를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[10]) # this의 임베딩 벡터값(eng_w2v에 저장되어져 있던 단어 'this'의 임베딩 벡터값과 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[10000]) # 단어 'self-indulgent'의 임베딩 벡터값(env_w2v에 단어 ''self-indulgent'의 임베딩 벡터값이 존재하지 않았기 때문에 'self-indulgent'의 임베딩 벡터값은 0으로 초기화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False) # 임베딩 벡터들이 저장되어져 있는 TEXT.vocab.vectors를 nn.Embedding()의 초기화 입력으로 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.LongTensor([10]))) # 단어 this의 임베딩 벡터값"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토치텍스트에서 제공하는 사전 훈련된 워드 임베딩"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치텍스트는 영어 단어들의 사전 훈련된 임베딩 벡터를 제공하고 있습니다. 다음은 제공되는 임베딩 벡터 리스트의 일부입니다.\n",
    "\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d\n",
    "- glove.twitter.27B.50d\n",
    "- glove.twitter.27B.100d\n",
    "- glove.twitter.27B.200d\n",
    "- glove.6B.50d\n",
    "- glove.6B.100d\n",
    "- glove.6B.200d\n",
    "- glove.6B.300d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe의 300차원의 임베딩 벡터들을 다운로드 받아 임베딩 벡터 초기화에 사용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT.build_vocab(trainset, vectors=GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[10]) # this의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors[10000]) # self-indulgent 임베딩 벡터값(사전 훈련된 임베딩 벡터의 단어장에 단어 'self-indulgent'이 존재하지 않았기 때문에 임베딩 벡터값은 0으로 초기화 된 상태)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
    "embedding_layer(torch.LongTensor([10])) # 단어 this의 임베딩 벡터값"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
