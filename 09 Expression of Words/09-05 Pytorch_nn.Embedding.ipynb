{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치에서는 임베딩 벡터를 사용하는 방법\n",
    "- nn.Embedding()를 사용해 임베딩 층(embedding layer)을 만들고 훈련데이터로부터 처음부터 임베딩 벡터를 학습\n",
    "- 사전에 훈련된 임베딩 벡터(pre-trained word embedding)들을 가져와 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 층은 룩업 테이블이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 → 단어 집합을 통해 정수 인코딩 → 임베딩 층(룩업 테이블) → 임베딩 벡터/밀집 벡터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 층 사용하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding의 인자들\n",
    "- num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기입니다.\n",
    "- embedding_dim : 임베딩 할 벡터의 차원입니다. 사용자가 정해주는 하이퍼파라미터입니다.\n",
    "- padding_idx : 선택적으로 사용하는 인자입니다. 패딩을 위한 토큰의 인덱스를 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "train_data = 'you need to know how to code'\n",
    "\n",
    "# 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "word_set = set(train_data.split())\n",
    "\n",
    "# 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "\n",
    "#  nn.Embedding() 사용\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab), \n",
    "                               embedding_dim=3,\n",
    "                               padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩업 테이블 출력\n",
    "print(embedding_layer.weight)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
