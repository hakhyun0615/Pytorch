{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토치텍스트 튜토리얼(Torchtext tutorial) - 영어"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치텍스트(Torchtext): 텍스트에 대한 여러 추상화 기능을 제공하는 자연어처리 라이브러리\n",
    "\n",
    "토치텍스트가 제공하는 기능들\n",
    "- 파일 로드하기(File Loading) : 다양한 포맷의 코퍼스를 로드합니다.\n",
    "- 토큰화(Tokenization) : 문장을 단어 단위로 분리해줍니다.\n",
    "- 단어 집합(Vocab) : 단어 집합을 만듭니다.\n",
    "- 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑합니다.\n",
    "- 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어줍니다. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수도 있습니다.\n",
    "- 배치화(Batching) : 훈련 샘플들의 배치를 만들어줍니다. 이 과정에서 패딩 작업(Padding)도 이루어집니다.\n",
    "\n",
    "토치텍스트가 모든 전처리를 해결해주지는 않습니다. 위 모든 과정 이전에 훈련 데이터, 검증 데이터, 테스트 데이터를 분리하는 작업은 별도로 해주어야 하며 위 모든 과정 이후에 각 샘플에 대해서 단어들을 임베딩 벡터로 맵핑해주는 작업도 별도로 해주어야 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터와 테스트 데이터로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
    "\n",
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1') # IMDB 리뷰 데이터는 영화 리뷰가 긍정인지 부정인지에 대한 데이터\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:25000] # 훈련데이터 25000개\n",
    "test_df = df[25000:] # 테스트데이터 25000개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_data.csv\", index=False) # 인덱스 저장 x\n",
    "test_df.to_csv(\"test_data.csv\", index=False) # 인덱스 저장 x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필드 정의하기(torchtext.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext.data에는 필드(Field)라는 도구를 제공합니다. 필드를 통해 앞으로 어떤 토큰화 전처리를 할 것인지를 정의합니다.\n",
    "\n",
    "- sequential : 시퀀스 데이터 여부. (True가 기본값)\n",
    "- use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)\n",
    "- tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)\n",
    "- lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)\n",
    "- batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)\n",
    "- is_target : 레이블 데이터 여부. (False가 기본값)\n",
    "- fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 지정된 프로시저를 찾을 수 없습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m data \u001b[39m# torchtext.data 임포트\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# 필드 정의\u001b[39;00m\n\u001b[0;32m      4\u001b[0m TEXT \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mField(sequential\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                   use_vocab\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m                   tokenize\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39msplit,\n\u001b[0;32m      7\u001b[0m                   lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m                   batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m                   fix_length\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[39m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(_get_torch_home(), \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _mod_utils\u001b[39m.\u001b[39mis_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchtext._torchtext\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtorchtext C++ Extension is not found.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m _load_lib(\u001b[39m\"\u001b[39;49m\u001b[39mlibtorchtext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torch\\_ops.py:104\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     99\u001b[0m path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils_internal\u001b[39m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m    100\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] 지정된 프로시저를 찾을 수 없습니다"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data # torchtext.data 임포트\n",
    "\n",
    "# 필드 정의\n",
    "TEXT = data.Field(sequential=True,\n",
    "                  use_vocab=True,\n",
    "                  tokenize=str.split,\n",
    "                  lower=True,\n",
    "                  batch_first=True,\n",
    "                  fix_length=20)\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   batch_first=False,\n",
    "                   is_target=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularDataset은 데이터를 불러오면서 필드에서 정의했던 토큰화 방법으로 토큰화를 수행합니다.\n",
    "\n",
    "- path : 파일이 위치한 경로.\n",
    "- format : 데이터의 포맷.\n",
    "- fields : 위에서 정의한 필드를 지정. 첫번째 원소는 데이터 셋 내에서 해당 필드를 호칭할 이름, 두번째 원소는 지정할 필드.\n",
    "- skip_header : 데이터의 첫번째 줄은 무시."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(train_data[0])) # vars()를 통해서 주어진 인덱스의 샘플을 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 집합(Vocabulary) 만들기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 전처리(완료) -> 단어 집합(현재) -> 정수 인코딩\n",
    "\n",
    "정의한 필드에 .build_vocab() 도구를 사용하면 단어 집합을 생성합니다.\n",
    "\n",
    "- min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가.\n",
    "- max_size : 단어 집합의 최대 크기를 지정.(사실 토치텍스트가 임의로 특별 토큰인 unk(단어 집합에 없는 단어)와 pad(패딩)를 추가해서 max_size+2가 된다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi) # .stoi를 통해 단어 집합 내의 단어들 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토치텍스트의 데이터로더 만들기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 전처리(완료) -> 단어 집합(완료) -> 정수 인코딩(현재)\n",
    "\n",
    "데이터로더는 데이터셋에서 미니 배치만큼 데이터를 로드하게 만들어주는 역할을 합니다. 토치텍스트에서는 Iterator를 사용하여 데이터로더를 만듭니다. 각 샘플은 더 이상 단어 시퀀스가 아니라 정수 시퀀스임을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 지정된 프로시저를 찾을 수 없습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterator\n\u001b[0;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      5\u001b[0m train_loader \u001b[39m=\u001b[39m Iterator(dataset\u001b[39m=\u001b[39mtrain_data, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[39m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(_get_torch_home(), \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _mod_utils\u001b[39m.\u001b[39mis_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchtext._torchtext\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtorchtext C++ Extension is not found.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m _load_lib(\u001b[39m\"\u001b[39;49m\u001b[39mlibtorchtext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\site-packages\\torch\\_ops.py:104\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     99\u001b[0m path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils_internal\u001b[39m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m    100\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[1;32mc:\\Users\\82105\\anaconda3\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] 지정된 프로시저를 찾을 수 없습니다"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = Iterator(dataset=train_data, batch_size=batch_size)\n",
    "test_loader = Iterator(dataset=test_data, batch_size=batch_size)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader)) # 첫번째 미니배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(batch)) # 일반적인 데이터로더는 미니 배치를 텐서로 가져오지만 토치텍스트의 데이터로더는 torchtext.data.batch.Batch 객체를 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.text) #  실제 데이터 텐서에 접근하기 위해서는 정의한 필드명을 사용해야 한다다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
